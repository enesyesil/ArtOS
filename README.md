# ArtOS â€” Research Prototype

**ArtOS** is a student-led research project exploring the design of an **LLM-aware operating layer** that can run large language models under strict security, anti-hallucination, and capability-scoped controls.

The goal is to treat an LLM **not as a trusted decision-maker**, but as an **untrusted coprocessor** operating within a controlled environment.  
This project investigates how operating system principles (capabilities, sandboxing, syscall mediation, auditing) can be adapted to the unique security and reliability challenges of LLM-based systems.

---

## ğŸ¯ Research Objectives
1. **Investigate hallucination-resistant architectures** for LLM-powered systems.
2. **Design and prototype a capability-based syscall interface** for LLMs.
3. **Integrate retrieval-anchored output validation** to ensure grounded answers.
4. **Evaluate sandboxing approaches** (e.g., WASM, microVMs) for LLM tools.
5. **Develop auditing and policy enforcement mechanisms** tailored for AI agents.

---

## ğŸ§© Research Questions
- How can traditional OS security models (e.g., capability systems, mandatory access control) be adapted to control LLM behaviour?
- What schema enforcement and fact-checking methods most effectively reduce hallucinations?
- How can we prevent LLMs from executing unsafe tool actions (e.g., prompt injection, SSRF) without overly restricting functionality?
- Can a syscall broker for LLMs be made general-purpose, supporting different AI models and domains?

---

## ğŸ“ Proposed Architecture (Prototype)

User Query  
â†“  
LLM Interface (Untrusted)  
â†“  
Syscall Broker (Go)  
  â€¢ Capability Check  
  â€¢ SchemaGuard  
  â€¢ FactGuard  
â†“  
Policy Engine (Rust)  
â†”  
Tool Sandbox (WASM)  
â†“  
Audit Ledger (Tamper-evident)

--- 


---

## ğŸ“… Research Plan & Milestones

### Phase 1 â€“ Literature Review (Weeks 1â€“2)
- Study existing OS capability systems and AI safety tooling.
- Review LLM security and hallucination mitigation research.

### Phase 2 â€“ Architecture Design (Weeks 3â€“4)
- Define syscall broker interface and capability token format.
- Draft JSON Schemas for tool inputs/outputs.
- Select sandboxing technology (WASM for MVP).

### Phase 3 â€“ Prototype Implementation (Weeks 5â€“8)
- Build broker (Go) with schema validation and audit logging.
- Implement policy engine (Rust) with basic allow/deny rules.
- Create two WASM tools (`retriever`, `kv`).

### Phase 4 â€“ Testing & Evaluation (Weeks 9â€“10)
- Simulate normal usage, malicious prompts, and injection attempts.
- Measure hallucination rate reduction vs. baseline LLM.

### Phase 5 â€“ Reporting (Weeks 11â€“12)
- Document architecture, implementation details, and test results.
- Identify limitations and future work.

---

## ğŸ›¡ï¸ Security Principles
1. **LLM is never root** â€” all actions require explicit, scoped capability tokens.
2. **Grounded output** â€” every answer must be supported by retriever-sourced citations.
3. **Schema-constrained communication** â€” outputs are validated before acceptance.
4. **Fail closed** â€” if validation or policy checks fail, the operation is blocked.

---

## ğŸ“œ License
MIT (Open for research and educational use)

